---
title: "Process Book"
author: "Jae Woong Ham & Andrew Thvedt"
date: "4/5/2020"
output: html_document
---

### Introduction

This is the process book for Group W. If you were looking for the main website, please refer back to thereadme page. In this section, we will walk you through how the direction of our project has developed from our original proposal to our final execution. Please keep in mind that this does not include all the code and visualizations we have worked on, but included the most important phases of our project that led up to our final design, in accordance with the final project instructions.


### Project Direction

Our project took a more defined direction than what we originally set for our preliminary group proposal. From considering multiple forms of transportation available, we decided to use the NYC taxi data alone. This is due to a number of factors. First, every publicly available data is severy gitabytes in size for just one year alone. Secondly, the structure of the data is not coherent from one transportation to another. As such, doing a technical comparison was not feasible, and such comparison would not be a fair evaluation. Thirdly, even if we still considering different forms of transportation, it would not provide much meaningful real-life insights. For example, the comparison of trip cost between trains and taxis would not be useful, as trains has a flat fee regardless of destination unless if someone were to switch train lanes. Simiarly, it wouldn't be useful to compare trip duration between taxis and ubers because that would mostly likely be dependent on traffic, and not the type of car they are driving. In other words, trip duration from either taxi data or ride sharing services should tell a very similar story for trip duration given a point in time.

With that in mind, our group determined that focuing on taxi data alone would provide more useful and interesting stories by diving deep into the data.


### NYC Taxi Data

Now that we have chosen the data, the first step in understanding our dataset is simply to load and preview the data.

```{r}
# importing all necessary packages
library(dplyr)
library(tidyverse)
library(ggplot2)
library(plotly)
library(RColorBrewer)
library(tibble)
library(lubridate)
```

```{r}
# importing data
jan_data <- read_csv("data/yellow_tripdata_2019-01.csv")
feb_data <- read_csv("data/yellow_tripdata_2019-02.csv")
mar_data <- read_csv("data/yellow_tripdata_2019-03.csv")
apr_data <- read_csv("data/yellow_tripdata_2019-04.csv")
may_data <- read_csv("data/yellow_tripdata_2019-05.csv")
jun_data <- read_csv("data/yellow_tripdata_2019-06.csv")
july_data <- read_csv("data/yellow_tripdata_2019-07.csv")
aug_data <- read_csv("data/yellow_tripdata_2019-08.csv")
sep_data <- read_csv("data/yellow_tripdata_2019-09.csv")
oct_data <- read_csv("data/yellow_tripdata_2019-10.csv")
nov_data <- read_csv("data/yellow_tripdata_2019-11.csv")
dec_data <- read_csv("data/yellow_tripdata_2019-12.csv")
```

Upon first glance, the dataset seems largely complete. However, since this dataset has millions of observations, our group wanted to calculate the number of missing observations to get a more accurate sense of this dataset.

```{r, eval = FALSE}
#total missingness in data
sum(is.na(oct_data))

#missingness by column
colMeans(is.na(oct_data))*100
```

In total, there are 233,615 missing observations in the data for october. Though this number seem large, there are two mitigating factors. First, the dataset contains over seven million observations. Second, on a column by column basis, however, the missing data is not too important for our project. The pickup time, dropoff time, pickup location, and dropoff location columns have 0 missing data. Most of the missingness is in the VendorID, store_and_fwd_flag, passenger_count, payment_type, and RatecodeID columns. This is not a big issue for our project, as these columns are less useful for us.


### Data Processing

Once we had explored the data and discussed the next steps, we were ready to begin processing the data. Even though our group has already chosen to focus on the nyc yellow cab data, it easily had over 6.5 million data points on average per given month. This meant that not only do we have to focus on one year alone, we had to subset the data even further given it sheer large size. We have chosen 2019, as this was the most recent year in which the data was available. This would provide the most up to date insights in relation to the variables of interest. Most importantly, we took the following steps in preprocessing the dat based on the following logic. Each point is demonstrated using the original July dataset prior to the preprocessing and subset procedure.

#### 1) Rate Code ID
Need to exclude rate code ID that fall out of the accepted code ID, and should be filtered for only the standard rate. For example, the July dataset clearly has a rate code ID that falls out of the acceptable range from 1-6, and also includes NA.
```{r}
unique(july_data$RatecodeID)
```

#### 2) Passenger Counts
The maximum number of passengers accepted in a yellow cab taxi is 4, and 5 for some vehicles. As such, the number of passengers above 5 should be excluded from the dataset. Evident from the example below, there are trips with 0 or more than 5 passengers, which have to be excluded.
```{r}
unique(july_data$passenger_count)
```

#### 3) Fare Amount
The fare amount includes negative numbers, and fares less than the mandatory $2.50 inital fee required by yellow cab drivers. As such, these observations should be dropped. As seen from the July dataset below, there are around 17000 observations with such cases.
```{r}
as.data.frame(table(july_data$fare_amount < 2.5)) %>%
  rename(
    Invalid_Fares = Var1
  )
```

#### 4) Total Amount
According to the official data dictionary, the total amount includes the tip amount. However, it excludes the tip amount if it is paid in cash. As such, the total amount should not include the tip amount altogether to compare more evenly across each trip. A new variable was created for a revised total amount.

#### 5) Fare by Distance
Our team thought it was useful to include an additional variable that calculates the cost of total fare by trip distance to understand how much customers are paying by mile.

#### 6) Average Miles per Hour
Our team also added an avg mph column to determine the average speed of the trip by first calculating the duration of the trip from subtracting the pickup time from dropoff time, and then diving the trip distance by the duration.


#### 7) Randomly subsetting data
Given the large size of the data (around 6-7 million observations per month), our team decided to use a smaller portion of the data by randomly subsetting the data by 100,000 observations per month. Moreover, a required sample size was calculated by using a population size of 6.5 million and desired confidence level of 95% and margin of error of 1% to derive a representative sample of 9,465. As such, using 100,000 observations is well above the recommended representative sample size.


### Global Function

We then converted this process into a global function, in order to be able to rapidly apply it to each month. Since we also plan to work with this dataset beyond the scope of this project, this allows us to simply reuse or adapt this function later. We each created a function and merged both our work for a final function.

```{r}
# global function to preprocess and subset data
data_transform <- function(df) {
  df <- df %>%
    na.omit() %>%
    filter(
      RatecodeID == 1 
      & trip_distance > 0 
      & fare_amount > 2.5 
      & passenger_count > 0 
      & passenger_count <= 5) %>%
    sample_n(100000) %>%
    mutate(
      fare_by_dist = total_amount / trip_distance,
      duration = ((tpep_dropoff_datetime - tpep_pickup_datetime)/60),
      avg_mph = (trip_distance/as.double(duration)/60),
      adj_total = total_amount - tip_amount
    )
  return(df)
}
```

We then merged all the dataframes, and exported the data so that we have a master dataframe to work with. Moreover, this was also for an efficiency purpose, as we did not want to overcrowd our global r environment.

```{r}
# applying function to preprocess and subset data
jan_rev <- data_transform(jan_data)
feb_rev <- data_transform(feb_data)
mar_rev <- data_transform(mar_data)
apr_rev <- data_transform(apr_data)
may_rev <- data_transform(may_data)
jun_rev <- data_transform(jun_data)
july_rev <- data_transform(july_data)
aug_rev <- data_transform(aug_data)
sep_rev <- data_transform(sep_data)
oct_rev <- data_transform(oct_data)
nov_rev <- data_transform(nov_data)
dec_rev <- data_transform(dec_data)

# merging all of the dataframes
all_months <- do.call("rbind", list(jan_rev, feb_rev, mar_rev, apr_rev, may_rev, jun_rev, july_rev, aug_rev, sep_rev, oct_rev, nov_rev, dec_rev))

# export data
write.csv(all_months, file = "jan_dec.csv", row.names = FALSE)
```

### Further Refining and Scrubbing the Dataset

However, our group found ourselves further refining the dataset. As you may know, the preprocessing stage takes up majority of the time for any data science projects. These steps are more minor in comparison to the last stage, and each step is annotated in the code block below.

```{r}
# Creating a separate column for hour of the day
all_months$hour <- hour(all_months$tpep_pickup_datetime)

# Creating a separate column for weekdays
all_months$day <- wday(all_months$tpep_pickup_datetime, label = TRUE, week_start = 1)

# Creating a separate column for months
all_months$month <- month(all_months$tpep_pickup_datetime, label = TRUE)

# Creating a separate column for the date
all_months$date <- date(all_months$tpep_pickup_datetime)

# Exclude values where duration is negative
all_months <- all_months %>%
  filter (duration > 0)

# Replace Inf values with 0
all_months <- all_months %>%
  mutate(avg_mph = ifelse(avg_mph == Inf, 0, avg_mph))
```

### Concluding Remarks
From this point, our group wanted to explore various variables of interest that help us tell a compelling story and provide the reader with important insights. We were fortunate that there weren't that many variables within the original dataset. As such, it was clear which variables to choose. These were trip duration, tip amount, trip fare, and trip distance, in additon to our custom variables mentioned earlier. Our final work included these variables of interest, and each variable was tailored to the most appropriate visualization given the context. Our final outputs can be found in our main website. Thank you.